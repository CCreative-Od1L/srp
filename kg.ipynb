{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''coding = utf-8'''\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the classifier is used to NER.\n",
    "## 当然这东西花了十多分钟随便搭起来的，简陋了一点\n",
    "class AbstractClassifier(object):\n",
    "        \n",
    "        def __init__(self,model_name,tokenizer_name,token_type):\n",
    "            self.classifier = pipeline(\"token-classification\",model=model_name,tokenizer=tokenizer_name)\n",
    "            self.token_type = token_type\n",
    "        # it will return a index_buf\n",
    "        # item in index_buf explain\n",
    "        # index_buf[0]: word start index\n",
    "        # index_buf[1]: word end index\n",
    "        # index_buf[2]: word types\n",
    "        def get_sentence_index(self,sentence):\n",
    "            ## 感兴趣的可以自行看看result的返回值，或许对你有所帮助\n",
    "            dic_result = self.classifier(sentence)\n",
    "\n",
    "            index_buf = []\n",
    "            count = -1\n",
    "            has_begin = False\n",
    "            for item in dic_result:\n",
    "                if( 'b-' in item['entity'].lower() ):\n",
    "                    has_begin = True\n",
    "                    count += 1\n",
    "                    index_buf.append([item['start'],item['end'],self.token_type])\n",
    "                if( 'i-' in item['entity'].lower() ):\n",
    "                    if(has_begin):\n",
    "                        index_buf[count][1] = item['end']\n",
    "                if( 'o' == item['entity'].lower() ):\n",
    "                    has_begin = False\n",
    "            return index_buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merg_or_append(entities, name, entity):\n",
    "    has_same = False\n",
    "\n",
    "    for key in entities:\n",
    "        if(key.lower() == name.lower()):\n",
    "            has_same = True\n",
    "\n",
    "            if( key != name ):\n",
    "                if('alias' not in entities[key]):\n",
    "                    entities[key]['alias'] = []\n",
    "                if( name not in entities[key]['alias']):\n",
    "                    entities[key]['alias'] += [name]\n",
    "            break\n",
    "    \n",
    "    if(not has_same):\n",
    "        entities[name] = entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 用来识别 疾病实体 的分类器\n",
    "disease_classifier = AbstractClassifier(model_name=\"alvaroalon2/biobert_diseases_ner\",\n",
    "                                        tokenizer_name=\"alvaroalon2/biobert_diseases_ner\",\n",
    "                                        token_type=\"disease\")\n",
    "## 用来识别 药物/化学品实体 的分类器\n",
    "chemical_classifier = AbstractClassifier(model_name=\"sschet/biobert_chemical_ner\",\n",
    "                                         tokenizer_name=\"sschet/biobert_chemical_ner\",\n",
    "                                         token_type=\"chemical\")\n",
    "## 用来识别 基因/蛋白质实体 的分类器\n",
    "genetic_classifier = AbstractClassifier(model_name=\"alvaroalon2/biobert_genetic_ner\",\n",
    "                                        tokenizer_name=\"alvaroalon2/biobert_genetic_ner\",\n",
    "                                        token_type=\"genetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割为句子（简陋版本）\n",
    "# 具体的思路就是读取内容，根据特别的符号来分割句子\n",
    "# 对于一些小数点的数据就会出现问题\n",
    "input_dir = os.path.abspath('.')\n",
    "sentences = []\n",
    "sentence = \"\"\n",
    "\n",
    "for root, dirs, files in os.walk(os.path.join(input_dir,'dataset\\\\txt\\\\')):\n",
    "    for file in files:\n",
    "        line = \"\"\n",
    "        fn = os.path.join(root,file)\n",
    "        isAppend = False\n",
    "        with open(fn,'r') as file_to_read:\n",
    "            while True:\n",
    "                line = file_to_read.readline()\n",
    "                if not line:\n",
    "                    ## 文件结尾退出文件\n",
    "                    break\n",
    "                else:\n",
    "                    ## 空格行自动跳过\n",
    "                    if( line == '\\n'):\n",
    "                        continue\n",
    "                    start_index = 0\n",
    "                    for index in range(len(line)):\n",
    "                        if(line[index] == '.' \n",
    "                           or line[index] == ';' \n",
    "                           or line[index] == '?'\n",
    "                           or line[index] == '!'):\n",
    "                            if( start_index == index ):\n",
    "                                start_index = index + 1\n",
    "                                continue\n",
    "                            if( isAppend ):\n",
    "                                isAppend = False\n",
    "                                sentence += line[start_index:index]\n",
    "                            else:\n",
    "                                sentence = line[start_index:index]\n",
    "                            start_index = index + 1\n",
    "                            sentences.append(sentence)\n",
    "                            sentence = \"\"\n",
    "                        if(line[index] == '\\n'):\n",
    "                            if( not isAppend ):\n",
    "                                isAppend = True\n",
    "                            else:\n",
    "                                sentence += ' '\n",
    "                            sentence += line[start_index:index]\n",
    "        \n",
    "        words = []\n",
    "        type_strs = []\n",
    "\n",
    "        ## Get article aentences\n",
    "        for sentence in sentences:\n",
    "            ## for each sentence\n",
    "            index_bufs = []  ## 用来存字符索引的\n",
    "            index_bufs.append(disease_classifier.get_sentence_index(sentence))\n",
    "            index_bufs.append(chemical_classifier.get_sentence_index(sentence))\n",
    "            index_bufs.append(genetic_classifier.get_sentence_index(sentence))\n",
    "\n",
    "            \n",
    "            ## 按照以下方式来打印识别到的实体以及类型\n",
    "            for index_buf in index_bufs:\n",
    "                for index_pair in index_buf:\n",
    "                    words.append(sentence[index_pair[0]:index_pair[1]]) \n",
    "                    type_strs.append(index_pair[2])\n",
    "               \n",
    "        empty_json = {}\n",
    "        entities = json.loads(json.dumps(empty_json))\n",
    "\n",
    "        assert(len(words) == len(type_strs))\n",
    "\n",
    "        for index in range(len(words)):\n",
    "            entity = {\n",
    "                \"type\":type_strs[index]\n",
    "            }\n",
    "            merg_or_append(entities,words[index],entity)\n",
    "\n",
    "        # print(entities)\n",
    "\n",
    "        with open(\"./ner_output/entities.json\",\"w\") as file:\n",
    "            json.dump(entities,file)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试句子\n",
    "sentence = \"In 1964, Blumberg and Alter collected blood samples from all over the world for the study of lipoprotein polymorphisms and serendipitously observed an unusual reaction between serumfrom a transfused hemophilic patient and an Australian aborigine, and the new antigen was designated as the “Australian antigen” [2, 3]\"\n",
    "## 用来存字符索引的\n",
    "index_bufs = []\n",
    "index_bufs.append(disease_classifier.get_sentence_index(sentence))\n",
    "index_bufs.append(chemical_classifier.get_sentence_index(sentence))\n",
    "index_bufs.append(genetic_classifier.get_sentence_index(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 按照以下方式来打印识别到的实体\n",
    "for index_buf in index_bufs:\n",
    "    for index_pair in index_buf:\n",
    "        print(\"word:\\\"\"+ sentence[index_pair[0]:index_pair[1]] + \"\\\" type:\" + index_pair[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': {'type': 'disease', 'alias': ['a1', 'a2']}}\n"
     ]
    }
   ],
   "source": [
    "test_json={\n",
    "    \"A1\":{\n",
    "        \"type\":\"disease\"\n",
    "    }\n",
    "}\n",
    "\n",
    "test_json[\"A1\"][\"alias\"] = []\n",
    "test_json[\"A1\"][\"alias\"] += [\"a1\"]\n",
    "test_json[\"A1\"][\"alias\"] += [\"a2\"]\n",
    "\n",
    "print(test_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
