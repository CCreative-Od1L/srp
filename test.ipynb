{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CodeField\\Codefile\\srp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n    About model param\\n    you can see https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward\\n    author: Well, I think it is no need to understand each params in this function. Because the model we use is \\n    trained by others.The only thing we ought to do is understanding the attribute of the output of the function.\\n\\n    About the return\\n    author: According to the official web page. The function will return \\n    \"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\" or \"tuple(torch.FloatTensor)\"\\n    which is comprised various elements depending on the configuration (BertConfig) and inputs.\\n\\n        output property list\\n        1. last_hidden_state(torch.FloatTensor of shape (batch_size, sequence_length, hidden_size))\\n            Sequence of hidden-states at the output of the last layer of the model.\\n        2. pooler_output(torch.FloatTensor of shape (batch_size, hidden_size))\\n            hidden-state of the first token of the sequence (classification token) after further processing \\n            through the layers used for the auxiliary pretraining task.\\n            author: [CLS] word_vector\\n        3. hidden_states (tuple(torch.FloatTensor)\\n            🔺 it is optional. returned when output_hidden_states=True is passed or when \\n            config.output_hidden_states=True)\\n            Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding \\n            layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size)\\n        4. attentions(tuple(torch.FloatTensor)\\n            🔺 it is optional. returned when output_attentions=True is passed or when \\n            config.output_attentions=True)\\n            Attentions weights after the attention softmax, used to compute the weighted average in the \\n            self-attention heads.\\n        5. cross_attentions (tuple(torch.FloatTensor)\\n            🔺 it is optional.returned when output_attentions=True and config.add_cross_attention=True is \\n            passed or when config.output_attentions=True)\\n            Attentions weights of the decoder\\'s cross-attention layer, after the attention softmax, used to \\n            compute the weighted average in the cross-attention heads.\\n        6. past_key_values (tuple(tuple(torch.FloatTensor))\\n            🔺 it is optional.returned when ...\\n            Contains pre-computed hidden-states(both optional. When...) that can be used (see past_key_values \\n            input) to speed up sequential decoding.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "About this you can see more information in the website: https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel\n",
    "'''\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "# return transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "''' \n",
    "    About model param\n",
    "    you can see https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward\n",
    "    author: Well, I think it is no need to understand each params in this function. Because the model we use is \n",
    "    trained by others.The only thing we ought to do is understanding the attribute of the output of the function.\n",
    "\n",
    "    About the return\n",
    "    author: According to the official web page. The function will return \n",
    "    \"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\" or \"tuple(torch.FloatTensor)\"\n",
    "    which is comprised various elements depending on the configuration (BertConfig) and inputs.\n",
    "\n",
    "        output property list\n",
    "        1. last_hidden_state(torch.FloatTensor of shape (batch_size, sequence_length, hidden_size))\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        2. pooler_output(torch.FloatTensor of shape (batch_size, hidden_size))\n",
    "            hidden-state of the first token of the sequence (classification token) after further processing \n",
    "            through the layers used for the auxiliary pretraining task.\n",
    "            author: [CLS] word_vector\n",
    "        3. hidden_states (tuple(torch.FloatTensor)\n",
    "            🔺 it is optional. returned when output_hidden_states=True is passed or when \n",
    "            config.output_hidden_states=True)\n",
    "            Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding \n",
    "            layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size)\n",
    "        4. attentions(tuple(torch.FloatTensor)\n",
    "            🔺 it is optional. returned when output_attentions=True is passed or when \n",
    "            config.output_attentions=True)\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the \n",
    "            self-attention heads.\n",
    "        5. cross_attentions (tuple(torch.FloatTensor)\n",
    "            🔺 it is optional.returned when output_attentions=True and config.add_cross_attention=True is \n",
    "            passed or when config.output_attentions=True)\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to \n",
    "            compute the weighted average in the cross-attention heads.\n",
    "        6. past_key_values (tuple(tuple(torch.FloatTensor))\n",
    "            🔺 it is optional.returned when ...\n",
    "            Contains pre-computed hidden-states(both optional. When...) that can be used (see past_key_values \n",
    "            input) to speed up sequential decoding.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hepatitis B virus is the pathogen that causes hepatitis B (hepatitis B for short).It belongs to the hepatophil DNA virus family, which contains two genera, hepaticophil DNA virus and avian hepaticophil DNA virus.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(sentence,\n",
    "                  return_tensors=\"pt\",\n",
    "                  padding=\"longest\")\n",
    "outputs = model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1124,  4163, 27659,   139,  7942,  1110,  1103,  3507, 19790,\n",
      "          1115,  4680,  1119,  4163, 27659,   139,   113,  1119,  4163, 27659,\n",
      "           139,  1111,  1603,   114,   119,  1135,  7017,  1106,  1103,  1119,\n",
      "          4163,  9870, 20473,  5394,  7942,  1266,   117,  1134,  2515,  1160,\n",
      "         11974,   117,  1119,  4163,  2941,  4184, 20473,  5394,  7942,  1105,\n",
      "           170, 10644,  1119,  4163,  2941,  4184, 20473,  5394,  7942,   119,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "attention_mask\n"
     ]
    }
   ],
   "source": [
    "for item in input:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next step is: \n",
    "根据目前的模型输出完成一定程度的下游工作\n",
    "Goal:\n",
    "完成实体的识别，并输出实体列表\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
