{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "About this you can see more information in the website: https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel\n",
    "'''\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "# return transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n",
    "model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "''' \n",
    "    About model param\n",
    "    you can see https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertModel.forward\n",
    "    author: Well, I think it is no need to understand each params in this function. Because the model we use is \n",
    "    trained by others.The only thing we ought to do is understanding the attribute of the output of the function.\n",
    "\n",
    "    About the return\n",
    "    author: According to the official web page. The function will return \n",
    "    \"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\" or \"tuple(torch.FloatTensor)\"\n",
    "    which is comprised various elements depending on the configuration (BertConfig) and inputs.\n",
    "\n",
    "        output property list\n",
    "        1. last_hidden_state(torch.FloatTensor of shape (batch_size, sequence_length, hidden_size))\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        2. pooler_output(torch.FloatTensor of shape (batch_size, hidden_size))\n",
    "            hidden-state of the first token of the sequence (classification token) after further processing \n",
    "            through the layers used for the auxiliary pretraining task.\n",
    "            author: [CLS] word_vector\n",
    "        3. hidden_states (tuple(torch.FloatTensor)\n",
    "            ðŸ”º it is optional. returned when output_hidden_states=True is passed or when \n",
    "            config.output_hidden_states=True)\n",
    "            Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding \n",
    "            layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size)\n",
    "        4. attentions(tuple(torch.FloatTensor)\n",
    "            ðŸ”º it is optional. returned when output_attentions=True is passed or when \n",
    "            config.output_attentions=True)\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the \n",
    "            self-attention heads.\n",
    "        5. cross_attentions (tuple(torch.FloatTensor)\n",
    "            ðŸ”º it is optional.returned when output_attentions=True and config.add_cross_attention=True is \n",
    "            passed or when config.output_attentions=True)\n",
    "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to \n",
    "            compute the weighted average in the cross-attention heads.\n",
    "        6. past_key_values (tuple(tuple(torch.FloatTensor))\n",
    "            ðŸ”º it is optional.returned when ...\n",
    "            Contains pre-computed hidden-states(both optional. When...) that can be used (see past_key_values \n",
    "            input) to speed up sequential decoding.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json(test)\n",
    "with open(\"./ner_output/test_w.json\",\"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"A1\": {\n",
      "        \"type\": \"disease\",\n",
      "        \"alias\": \"a1\"\n",
      "    },\n",
      "    \"B2\": {\n",
      "        \"type\": \"disease\"\n",
      "    },\n",
      "    \"C1\": {\n",
      "        \"type\": \"disease\",\n",
      "        \"alias\": \"c1\"\n",
      "    },\n",
      "    \"A2\": {\n",
      "        \"type\": \"disease\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\n",
      "B2\n",
      "C1\n",
      "A2\n"
     ]
    }
   ],
   "source": [
    "for key in data:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merg_or_append(entities, name, entity):\n",
    "    has_same = False\n",
    "\n",
    "    for key in entities:\n",
    "        if(key.lower() == name.lower()):\n",
    "            has_same = True\n",
    "            entities[key]['alias'] = name\n",
    "            break\n",
    "    \n",
    "    if(not has_same):\n",
    "        entities[name] = entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write json(complete)\n",
    "\n",
    "names = [\"A1\",\"B2\",\"C1\",\"A2\",\"a1\",\"c1\"]\n",
    "type = \"disease\"\n",
    "empty_json = {}\n",
    "entities = json.loads(json.dumps(empty_json))\n",
    "\n",
    "for name in names:\n",
    "    entity = {\n",
    "        \"type\":type\n",
    "    }\n",
    "    merg_or_append(entities,name,entity)\n",
    "\n",
    "# print(entities)\n",
    "\n",
    "with open(\"./ner_output/test_w.json\",\"w\") as file:\n",
    "    json.dump(entities,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write json(test)\n",
    "\n",
    "entity = {\n",
    "    \"name\":\"A1\",\n",
    "    \"typy\":\"disease\"\n",
    "}\n",
    "with open(\"./ner_output/test_w.json\",\"w\") as file:\n",
    "    json.dump(entity,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
